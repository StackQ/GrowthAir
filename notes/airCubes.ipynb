{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmatically Author, Schedule and Monitor workflows.  \n",
    "Benefits:-\n",
    "- Atomic\n",
    "- Scalability\n",
    "- UI\n",
    "- Extensibility  \n",
    "\n",
    "**Core Components**\n",
    "- Web Server: Flast server with Gunicorn serving the UI. communicates with the 'Metastore', and 'Scheduler' & 'Executor' also communicates with 'Metastore'\n",
    "- Scheduler: Daemon in charge of scheduling workflows\n",
    "- Metastore\n",
    "- Triggerer\n",
    "- Executor: doesn't execute a task, but it defines how and on which system your tasks are executed. Sends tasks to execute to the 'Queue'\n",
    "- Queue: to execute tasks in the correct order\n",
    "- Worker\n",
    "\n",
    "**Operator** -> A way to encapsulate what we want to do\n",
    "> db= connect(host, credentials)  \n",
    "> db.insert(sql_request)\n",
    "- Action Operators : executes something, ex- PythonOperator executes a Python function, bash Operator executes bash commands\n",
    "- Transfer Operators : allow to transfer data from a point A to B, ex- MySQL to RedShift\n",
    "- Sensor Operators : allow to wait for something before moving to next task\n",
    "\n",
    "**Tasks** -> Operator is a task and when we run an operator, which is a \"task\" we get a task instance\n",
    "\n",
    "**Architecture**\n",
    "- Single Node Architecture = Node(Web Server + Metastore + Scheduler + Executor + Queue)\n",
    "- Multi Node Architecture = \n",
    "  - Case Celery Executore and have RabbitMQ/Redis\n",
    "    - Node 1: Web Server + Scheduler & Executor\n",
    "    - Node 2: Metastore + Queue\n",
    "    - 3/Many additional Worker Node\n",
    "\n",
    "**How does it works?**\n",
    "- Create a new DAG, dag.py and put that file into the folder \"dag\"\n",
    "- Scheduler process this folder ***dag*** every 5 mintues, by default to detect new DAGs.\n",
    "- Whenever we apply a modification to that DAG we may ned to wait up to 30 seconds before getting modification.\n",
    "  Adding new DAG Scheduler process every 5 minutes for new DAG and 30 seconds for existing DAG \n",
    "- Scheduler run the DAG and for that it creates a ***DagRun Object*** with the state=running\n",
    "- Then it takes the first task to execute and that tasks becomes a task instance object, it has the state=None and then Scheduled\n",
    "- After that the Scheduler sends the task instance object to the Queue of the Executor \n",
    "- Now the state of the task is Queued and the Executor creates a sub process to run the task, and now the task instance object has the state Running.\n",
    "- Once the task is Done, the state of the task is Success/Failed\n",
    "- if DAG is done then DAG Run has the state success.\n",
    "\n",
    "**Key Takeaways**\n",
    "- Airflow is an orchestrator, not a processing framework. Process GB of data outside of Airflow(i.e Have a Spark cluster, use an operator to execute a Spark Job and data is processed in Spark)\n",
    "- A DAG is a data pipeline, an Operator is a task\n",
    "- An executor defines how tasks are executed, whereas a worker is a process executing the tasks\n",
    "- The Scheduler schedules our tasks, the web server serves the UI, and the database stores the metadata of Airflow.\n",
    "\n",
    "**Providers** - https://registry.astronomer.io/  \n",
    "- Airflow Core : pip install apache-airflow\n",
    "- Snowflake : pip install apache-airflow-providers-snowflake\n",
    "- AWS : pip install apache-airflow-providers-amazon\n",
    "- Databricks : pip install apache-airflow-providers-databricks\n",
    "- Dbt : pip install apache-airflow-providers-dbt-cloud\n",
    "\n",
    "**DAG Scheduling**  \n",
    "- start_date: The timestamp from which the scheduler will attempt to backfill\n",
    "- schedule_interval: How often a DAG runs\n",
    "- end_date: The timestamp from which a DAG ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The New Way of Scheduling DAGs - DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Dataset?**\n",
    "A logical grouping of data, like a file, like a square table. Anything that has data, it has 2 properties:  \n",
    "- URI : the path to data/data file OR \n",
    "  - unique identifier of our data\n",
    "  - Path to data\n",
    "  - must composed of only ASCII characters\n",
    "  - The URI scheme can not be airflow\n",
    "  - case sensitive\n",
    "  - ex :\n",
    "  `from airflow import Dataset`  \n",
    "  `# valid datasets:`  \n",
    "  `schemaless = Dataset(\"/path/file.txt\")  `  \n",
    "  `csv_file = Dataset(\"file.csv\", extra={'owner': 'Raj'})`  \n",
    "  `#invalid datasets:  `  \n",
    "  `reserved = Dataset(\"airflow://file.txt\")`  \n",
    "  `not_ascii = Dataset(\"file_dataset\")`  \n",
    "  `\n",
    "- EXTRA : dictionary that we can define as {'owner' : 'owner of table is rshukla}, to attach additional information to our data set.\n",
    "\n",
    "**Dataset Limitations**  Dataset are amazing, but they have limitations as well:  \n",
    "- DAGs can only use Datasets in the same Airflow instance. A DAG cannot wait for a Dataset defined in another Airflow instance\n",
    "- Consumer DAGs are triggered every time a task that updates datasets completes successfuly. ***Airflow doesn't check whether the data has been effectively updated***\n",
    "- You can't combine diff schedules like datasets with cron expressions.\n",
    "- If two tasks update the same dataset, as soon as one is done, that triggers the Consumer DAG immediately without waiting for the second task to complete.\n",
    "- Airflow monitors datasets only within the context of DAGs and Tasks. if an external tool updates the actual data represented by a Dataset, Airflow has no way of knowing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "producer.py"
    ]
   },
   "outputs": [],
   "source": [
    "from airflow import DAG, Dataset\n",
    "from airflow.decorators import task\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "my_file = Dataset(\"/tmp/my_file.txt\")\n",
    "my_file_2 = Dataset(\"/tmp/my_file_2.txt\")\n",
    "\n",
    "with DAG(dag_id=\"producer\", schedule=\"@daily\", start_date=datetime(2023,11,20), catchup=False):\n",
    "\n",
    "    @task(outlets=[my_file])\n",
    "    def update_dataset():\n",
    "        with open(my_file.uri, \"a+\") as f:\n",
    "            f.write(\"producer update\")\n",
    "\n",
    "    @task(outlets=[my_file_2])\n",
    "    def update_dataset_2():\n",
    "        with open(my_file_2.uri, \"a+\") as f:\n",
    "            f.write(\"producer update\")            \n",
    "\n",
    "    update_dataset() >> update_dataset_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "consumer.py"
    ]
   },
   "outputs": [],
   "source": [
    "from airflow import DAG, Dataset\n",
    "from airflow.decorators import task\n",
    "from datetime import date, datetime\n",
    "\n",
    "my_file=Dataset(\"/tmp/my_file.txt\")\n",
    "my_file_2=Dataset(\"/tmp/my_file_2.txt\")\n",
    "\n",
    "with DAG(dag_id=\"consumer\", schedule=[my_file, my_file_2], start_date=datetime(2023,11,20), catchup=False):\n",
    "    \n",
    "    @task\n",
    "    def read_dataset():\n",
    "        with open(my_file.uri, \"r\") as f:\n",
    "            print(f.read())\n",
    "\n",
    "    read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Databases and Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's an executor?** It doesn't run our tasks, it doesn't execute our tasks. It defines how to run our tasks on which system, may diff executors we have that we can use:  \n",
    "- local executors : to run multiple tasks on a single machine\n",
    "  - sequential executor : to run one task at a time on a single machine\n",
    "- remote executors\n",
    "  - celery executor\n",
    "  - kubernetes executor  \n",
    "\n",
    "To change the executor, change executor parameter in the configuration file of airflow.  \n",
    "file->docker-compose.yml  \n",
    "`environment:  `  \n",
    "`  &airflow-common-env`.   \n",
    "`  AIRFLOW__CORE__EXECUTOR: CeleryExecutor  -> will override the setting defined `  \n",
    "  \n",
    "`file->airflow.cfg`  \n",
    "`executor = SequentialExecutor  `  \n",
    "\n",
    "***So as we use docker, we need to modify the configuration of airflow***\n",
    "\n",
    "**The Sequential Executor** By default when we install airflow manually, there are 3 components:\n",
    "- Web Server\n",
    "- Scheduler\n",
    "- SQLite\n",
    "\n",
    "**The Local Executor** One setp further than the sequential executor, as it allows to execute multiple tasks at the same time, but on a single machine and with diff database\n",
    "- Web Server\n",
    "- Scheduler\n",
    "- Postgres/MySQL/Oracle DB  \n",
    "to configure it  \n",
    "`executor=LocalExecutor`  \n",
    "`sql_alchemy_conn=postgresql+psycopg2://<user>:<password>@<host>/<db>`\n",
    "\n",
    "**The Celery Executor**\n",
    "- Web Server\n",
    "- Scheduler\n",
    "- Postgres/MySQL/Oracle DB  (metadata)\n",
    "- Celery Queue = Result backend(status of the tasks that have been executed) + Broker/Queue(could be Redis/RabbitMQ need to install, and coz of it 2 additional config need to define)\n",
    "- Many workers : to execute our tasks  \n",
    "to configure it  \n",
    "`executor=CeleryExecutor`  \n",
    "`sql_alchemy_conn=postgresql+psycopg2://<user>:<password>@<host>/<db>`  \n",
    "`celery_result_backend=postgresql+psycopg2://<user>:<password>@<host>/<db>`  \n",
    "`celery_broker_url=redis://:@redis:6379/0`\n",
    "\n",
    "**Config**\n",
    "- `airflow celery worker` executing this command on a given machine becomes a airflow worker where we can execute tasks\n",
    "- `docker cp growthair_airflow-scheduler_1:/opt/airflow/airflow.cfg .` to copy airflow configuration file to local\n",
    "- `docker-compose down && docker-compose --profile flower up -d` to start airflow flower\n",
    "- `https://stunning-fortnight-7xqxvp6pvr7fx594-5555.app.github.dev/` port 5555 to see number of celery/other workers and their info\n",
    "- AIRFLOW__CORE__LOAD_EXAMPLES=false to disable default DAGs example visibility\n",
    "\n",
    "**What is a Queue?**  \n",
    "CELERY = [Result Backend + Queue] => Queue(high_cpu | ml_model | default) =>where (high_cpu=Worker(5cpu) | ml_model=Worker(GPU) | default=Worker(1 cpu))\n",
    "\n",
    "**Add a new celery Worker**\n",
    "- docker-compose.yml -> services: -> copy & paste with 'airflow-worker' section and rename it as 'airflow-worker-2' & restart instance(`docker-compose down && docker-compose --profile flower up -d`), we will see 2 worker in 'flower' UI 5555  \n",
    "***OR***\n",
    "- if have multiple machine then on every machine run- `airflow celery worker` to consider that machine as airflow worker\n",
    "\n",
    "**Create a queue to better distribute tasks**\n",
    "- `airflow celery worker -q high_cpu` -> will create a new queue 'high_cpu' and will see it in flower and will be able to send the tasks only this specific worker using this queue high_cpu in docker-compose.yml section airflow-worker-2 in my case\n",
    "- by default when we execute a task, that task is sent to the default queue as defined in configuration using parameter *default_queue* in airflow.cfg \n",
    "\n",
    "**Concurrency** Defines the number of tasks and DAG Runs that we can execute at the same time(in parallel), Airflow has several parameters to tune tasks and DAGs concurrency  \n",
    "- parallelism/AIRFLOW__CORE__PARALLELISM: The MAX number of task instances can in Airflow per Scheduler. By default = 32 tasks at the same time, if 2 schedulers then 2 x 32 = 64 tasks\n",
    "- max_active_tasks_per_dag/AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: MAX number of task instances allowed to run concurrently in each DAG, by default 16 at the same time for a given DAG across all DAG run\n",
    "- max_active_runs_per_dag/AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: defines the MAX number of active DAG runs per DAG. By default can have up to 16 DAG runs per DAG running at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     " docker-compose -f docker-compose-es.yaml up -d"
    ]
   },
   "source": [
    "Resolved using following steps:\n",
    "\n",
    "If you want to run docker as non-root user then you need to add it to the docker group.\n",
    "\n",
    "Create the docker group if it does not exist: $ sudo groupadd docker\n",
    "\n",
    "Add your user to the docker group: $ sudo usermod -aG docker $USER\n",
    "\n",
    "Run the following command or Logout and login again and run (that doesn't work you may need to reboot your machine first) : $ newgrp docker\n",
    "\n",
    "Check if docker can be run without root: $ docker run hello-world\n",
    "\n",
    "Reboot if still got error: $ reboot\n",
    "\n",
    "Stop VM and restart and it will work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
